{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with imports\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, utils, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from Run import RunBuilder as RB\n",
    "from Run import RunManager as RM\n",
    "from DataLoading import UdacityDataset as UD\n",
    "from DataLoading import ConsecutiveBatchSampler as CB\n",
    "\n",
    "from model import TransferLearning as TL\n",
    "\n",
    "%run Visualization.ipynb\n",
    "\n",
    "torch.set_printoptions(linewidth=120) \n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Done with imports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = '/home/ortizalejandro93/data/train_data/' #'/home/ortizalejandro93/data/train_data_small/'\n",
    "csv_file_path = os.path.join(train_root, 'interpolated.csv')\n",
    "camera_type = 'center_camera'\n",
    "output_dir = '/home/ortizalejandro93/models/TL_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = OrderedDict(\n",
    "    learning_rate = [0.001],\n",
    "    batch_size = [50], # 50 wokred originally was 50 but threw the RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB ...\n",
    "    num_workers = [1],\n",
    "    #shuffle = [True,False]\n",
    ")\n",
    "\n",
    "m = RM.RunManager()\n",
    "for run in RB.RunBuilder.get_runs(parameters):\n",
    "    network = TL.TLearning()\n",
    "    network.cuda()\n",
    "    network.to(device)\n",
    "    optimizer = optim.Adam(network.parameters(),lr = run.learning_rate,betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001/run.batch_size, amsgrad=False)\n",
    "# modify the time to visible time format, also use it to check the sequency of pictures is from former to current\n",
    "    udacity_dataset = UD.UdacityDataset(csv_file=csv_file_path,\n",
    "                                     root_dir=train_root,\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera=camera_type)\n",
    "    dataset_size = int(len(udacity_dataset))\n",
    "    del udacity_dataset\n",
    "    split_point = int(dataset_size * 0.8)\n",
    "\n",
    "    training_set = UD.UdacityDataset(csv_file=csv_file_path,\n",
    "                                     root_dir=train_root,\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera=camera_type,\n",
    "                                     slice_frames =None, # remove 10 once done debugging run\n",
    "                                     select_range=(0,split_point))\n",
    "\n",
    "    validation_set = UD.UdacityDataset(csv_file=csv_file_path,\n",
    "                                     root_dir=train_root,\n",
    "                                     transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                     select_camera=camera_type,\n",
    "                                     slice_frames=None, # remove 10 once done debugging run\n",
    "                                     select_range=(split_point,dataset_size))\n",
    "    print(\"size of training set :{}\".format(len(training_set)))\n",
    "    print(\"size of validation set :{}\".format(len(validation_set)))\n",
    "    m.training_set = training_set\n",
    "    m.validation_set = validation_set\n",
    "    \n",
    "    training_cbs = CB.ConsecutiveBatchSampler(data_source=training_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=1)\n",
    "    training_loader = DataLoader(training_set, sampler=training_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "\n",
    "    validation_cbs = CB.ConsecutiveBatchSampler(data_source=validation_set, batch_size=run.batch_size, shuffle=True, drop_last=False, seq_len=1)\n",
    "    validation_loader = DataLoader(validation_set, sampler=validation_cbs, num_workers=run.num_workers, collate_fn=(lambda x: x[0]))\n",
    "\n",
    "    m.begin_run( run,network,[run.batch_size,3,224,224] )\n",
    "    for epoch in range(10): # was 10 limit to 1 epochs during debugging\n",
    "        m.begin_epoch()\n",
    "        for training_sample in tqdm(training_loader):\n",
    "            training_sample['image'] = training_sample['image'].squeeze()\n",
    "            training_sample['image'] = torch.Tensor(resize(training_sample['image'], (run.batch_size,3,224,224),anti_aliasing=True))\n",
    "\n",
    "            param_values = [v for v in training_sample.values()]\n",
    "            image,angle = param_values[0],param_values[3]\n",
    "            image = image.to(device) # RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.DoubleTensor) should be the same\n",
    "            prediction = network(image)\n",
    "            prediction = prediction.to(device)\n",
    "            labels = angle.to(device)\n",
    "            del param_values, image, angle\n",
    "            if labels.shape[0]!=prediction.shape[0]:\n",
    "                prediction = prediction[-labels.shape[0],:]\n",
    "            training_loss_angle = F.mse_loss(prediction.float(),labels.float(),size_average=None, reduce=None, reduction='mean')\n",
    "            optimizer.zero_grad()# zero the gradient that are being held in the Grad attribute of the weights\n",
    "            training_loss_angle.backward() # calculate the gradients\n",
    "            optimizer.step() # finishing calculation on gradient \n",
    "        print(\"Done\")\n",
    "# Calculation on Validation Loss\n",
    "        with torch.no_grad():    \n",
    "            for Validation_sample in tqdm(validation_loader):\n",
    "                Validation_sample['image'] = Validation_sample['image'].squeeze()\n",
    "                Validation_sample['image'] = torch.Tensor(resize(Validation_sample['image'], (run.batch_size,3,224,224),anti_aliasing=True))\n",
    "\n",
    "                param_values = [v for v in Validation_sample.values()]\n",
    "                image,angle = param_values[0],param_values[3]\n",
    "                image = image.to(device)\n",
    "                prediction = network(image)\n",
    "                prediction = prediction.to(device)\n",
    "                labels = angle.to(device)\n",
    "                del param_values, image, angle\n",
    "                if labels.shape[0]!=prediction.shape[0]:\n",
    "                    prediction = prediction[-labels.shape[0],:]\n",
    "                validation_loss_angle = F.mse_loss(prediction,labels,size_average=None, reduce=None, reduction='mean')\n",
    "                m.track_loss(validation_loss_angle)\n",
    "                m.track_num_correct(prediction,labels) \n",
    "        #m.end_epoch(validation_set)\n",
    "        m.end_epoch()\n",
    "        torch.save(network.state_dict(), os.path.join(output_dir, \"TL_Model-epoch-{}\".format(epoch)))\n",
    "    m.end_run()\n",
    "m.save('result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Directly from disk\n",
    "tl_model = TL.TLearning().to(device) # added import TL.\n",
    "tl_model.load_state_dict(torch.load(os.path.join(output_dir, 'TL_Model-epoch-1'))) # epoch 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_cnn(tl_model.ResNet.conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UD.Ud...\n",
    "udacity_dataset = UD.UdacityDataset(csv_file=csv_file_path, \n",
    "                                 root_dir=train_root,\n",
    "                                 transform=transforms.Compose([transforms.ToTensor()]),\n",
    "                                 select_camera=camera_type)\n",
    "\n",
    "\n",
    "# Load arbitrary data\n",
    "# Needs debugging seeing: TypeError: can't convert cuda:0 device type tensor to numpy. \n",
    "# Use Tensor.cpu() to copy the tensor to host memory first.\n",
    "\n",
    "sample = udacity_dataset[3693] # a dict with 'image' tensor of torch.Size([3, 480, 640]), etc.\n",
    "#sample['image'] = torch.unsqueeze(sample['image'].cuda(), 0)\n",
    "#import pdb; pdb.set_trace()\n",
    "sample['image'] = torch.Tensor(resize(sample['image'], (1,3,480,640),anti_aliasing=True)).cuda()\n",
    "pred_angle = tl_model(sample['image']) # pass sample through network to get predicted angle\n",
    "#import pdb; pdb.set_trace()\n",
    "sample['image'] = sample['image'].cpu()\n",
    "show_sample(sample, pred_angle) # added second arg to run\n",
    "input_image = sample['image'].reshape(-1, 3, 480, 640).cuda()\n",
    "\n",
    "\n",
    "cam_extractor_tl = CamExtractorTLModel(tl_model)\n",
    "\n",
    "# Forward pass\n",
    "model_output = tl_model(input_image)\n",
    "\n",
    "# Backward pass\n",
    "tl_model.zero_grad()\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(model_output, sample['angle'].cuda().reshape(1,1).float()) # sample metrics are doubles\n",
    "loss.backward()\n",
    "\n",
    "cam_image = cam_extractor_tl.to_image(height=480, width=640) # Use this line to extract CAM image from the model!\n",
    "plt.imshow(cam_image[0, :, :], cmap='jet', alpha=0.5) # this shows CAM as overlay to the original input image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "09da5c0f0cd944e8a1b92991cb83ad32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "169fc6dcfbf1453bb3f993e8f820fb75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "bar_style": "danger",
       "description": "  1%",
       "layout": "IPY_MODEL_cf39cd2fed3f4160b56b3648d03a035d",
       "max": 541,
       "style": "IPY_MODEL_cdb7f50858604b57ac7918c821d5fc76",
       "value": 4
      }
     },
     "231c4cfb7acf4255bf00331b651e4d23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8903b5f73cfb4e1d8c1b58a76a110ce8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_169fc6dcfbf1453bb3f993e8f820fb75",
        "IPY_MODEL_97870d47c3244e6b97dd316925455a01"
       ],
       "layout": "IPY_MODEL_a8e1b14a27fd48b587e88a8cd3d24a86"
      }
     },
     "97870d47c3244e6b97dd316925455a01": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_231c4cfb7acf4255bf00331b651e4d23",
       "style": "IPY_MODEL_09da5c0f0cd944e8a1b92991cb83ad32",
       "value": " 4/541 [00:09&lt;17:54,  2.00s/it]"
      }
     },
     "a8e1b14a27fd48b587e88a8cd3d24a86": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cdb7f50858604b57ac7918c821d5fc76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "cf39cd2fed3f4160b56b3648d03a035d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
